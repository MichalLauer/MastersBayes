The important characteristic of MCMC methods, such as HA, MHA, HMC, or NUTS, is that they converge to the true
posterior distribution. If the convergence is fast and accurate, generated samples are representative enough for
statistical inference. The sampling speed might sometimes be problematic, and even with the most advanced samplers, the
inference might take a relatively long time. The speed may be more crucial than the accuracy, and researchers may
prefer speed and quick inference with no guarantees that the model represents the exact posterior distribution. This
requirement can be seen in real-time data analytics or on-demand predictions, where speed is significantly more
important than accuracy.

The issue can be solved with samplers based on Variational Inference (VI). Variational inference transforms the
sampling problem from a stochastic process to an optimization task. Instead of sampling many samples whose stationary
distribution is the true posterior, VI tries to find a distribution close to the posterior distribution measured by a
Kullback–Leibler Divergence (KLD). It is a measure of how two probability distributions are similar to each other.
Bayesian statistics measures the \enquote{distance} between the true posterior $P(\theta | x)$ and arbitrary
distribution function $q$. Interestingly, KLD naturally raises in many statistical methods. One example can be the
maximum likelihood estimation that naturally minimizes the KLD \parencite{ranganath2017black}.

\parencite{KullbackLeibler1951_InformationSufficiency} define KLD as

$$
KL(f_1(x) || f_2(x)) = \int f_1(x) \log \left( 
    \frac{f_1(x)}{f_2(x)}
\right) d x
$$ {#eq-kld}

Applying Equation @eq-kld to IV and reversing the fraction in $\log (\cdot)$[^log-reverse], the equation transforms to

[^log-reverse]: Because $\log (a/b) = -\log(b/a)$

$$
\text{KL}(q(\theta) || P(\theta || x)) = - \int q(\theta) \log \left( 
    \frac{P(\theta | x)}{q(\theta)}
\right) d \theta.
$$ {#eq-kld-bayes}

The smaller the KLD in Equation @eq-kld-bayes is, the closer the proposed distribution $q$ to the true posterior
$P(\theta|x)$ becomes. Many different measures could be used. However, it can be shown (see e.g.
\parencite{Sjolund2023_TutorialParametricVariational}) that with KLD, the predictive probability $P(x)$, which is
constant for each model, can be rewritten to

$$
\log P(x) = \text{ELBO}(q(\theta)) + \text{KL}(q(\theta) || P(\theta || x)),
$$

where ELBO denotes the evidence lower bound[^elbo]

[^elbo]: That is because quantity $P(x)$ is often called the *evidence*. Similarly, it could be called the predictive
lower bound, but that is not common.

$$
\text{ELBO}(q(\theta)) = \mathbb{E}_{q(\theta)} \left[ \log (x | \theta) - \log q(x) \right].
$$ {#eq-elbo}

Because the KLD is always non-negative, maximizing $\text{ELBO}(q(\theta))$ is the same as minimizing KLD. This
relationship is useful because minimizing Equation @eq-elbo is easier than Equation @eq-kld-bayes.
\parencite{Sjolund2023_TutorialParametricVariational} provides examples that show how the calculations are done in
practice.

The probability function $q(x)$ depends on hyperparameters that change how the distribution looks and
\parencite[Chap. 13.7]{Gelman2014_BayesianDataAnalysis} uses notation $q(x|\phi)$, where $\phi$ denotes such
hyperparameters. VI aims to find a function $q$ with hyperparameters $\phi$  
closest to the true posterior distribution $P(\theta | x)$. Then, new samples are taken from $q(\theta | \phi)$, and
classical inference can be conducted. The ideal lower limit of KLD is zero; thus, the distance can get arbitrarily
close.

There are various ways how to define distribution $q$. If the parameters $\theta$ are assumed to be independent, then

$$
g(\theta | \phi) = \prod_{j=1}^d g_j(\theta_j | \phi_j).
$$

Detailed process of estimation and the proof that the KLD is decreased with time is provided in
\parencite[Chap. 13.7]{Gelman2014_BayesianDataAnalysis}.

Even though the function $g(\theta | \phi)$ might not be exact, VI is already widely used in research projects and many
machine learning applications. \parencite{GefangEtAl2019_VariationalBayesianInference} use VI and Bayesian shrinkage to
develop a new method how estimate *\enquote{Vector Autoregressive models with hundreds of macroeconomic variables, or
more}*. The main motivation for VI in this specific paper is the complexity and computational infeasibility of
traditional MCMC methods. \parencite{MurakamiEtAl2025_RapidComprehensiveSearch} use Bayesian analysis and VI to
identify different crystal phases from X-ray diffraction data and their full profile. The paper's main goal is to find
a Bayesian method that utilizes VI to reduce the estimation time *\enquote{from a few hours to seconds}*.

VI is not the only sampling method that offers quick inference with only approximate results. Another method is
Laplace's approximation, which tries to center the Gaussian distribution at the maximum poster estimate using
second-order Taylor expansion. Its properties and stability are discussed in
\parencite{BarberEtAl2016_LaplaceApproximationHighdimensional}.
