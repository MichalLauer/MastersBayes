Bayesian statistics defines a degree of belief that might be unique to every researcher. It represents the conditional
probability distribution of estimated parameters $\theta$ given observed data $x$. This probability distribution is
called the posterior distribution, which can be computed using the Bayes' theorem
\parencite[Page 35]{Marek2012_Pravdepodobnost} as

$$
P(\theta | x ) = \frac{P(\theta)P(x|\theta)}{P(x)}.
$$ {#eq-bayes-eq}

First is the prior degree of belief that is individual and is set before observing any new data. One can create their
prior belief on common sense, past research, or domain knowledge. This initial belief is described with a probability
distribution called the prior distribution, denoted as $P(\theta)$ where $P$ denotes either probability or density
function and $\theta$ the unknown parameters. The second main component is based on the collected data and their
likelihood, given the prior degree of belief. The likelihood, denoted as $P(x|\theta)$, represents the conditional
probability of data $x$ given the prior belief about $\theta$. Given some prior beliefs, some data is more likely to
occur than others. This combination of prior degree of belief and observed data allows researchers to update their
degree of belief based on how it agrees with observations. The final component is the predictive probability, denoted
as $P(x)$ \parencite[Chap. 6]{Hebak2013_StatistickeMysleniNastroje}.

In research, it is often more suitable to talk about data and hypotheses. Equation @eq-bayes-eq is often rewritten to

$$
P(H|D) = \frac{P(H)P(D|H)}{P(D)},
$$

However, the interpretation is identical. In practice, Equation @eq-bayes-eq is composed of probability functions and
integrals.

The choice of prior belief been historicaly limited by the tractability of the model. Before sampling software has been
adopted, models needed to be conjugate. This means that combinig the likelihood function and the prior distribution
gives rise to a posterior distribution that has known analytical solution. In the past, selection of specific functions
and distributions that are conjugate has been favorable because of their simplicity. If there are better functions or
distribution for the analysis that are not conjugate, the posterior distribution might not have tractable solution and
thus the analysis becomes difficult. General definition of conjugate prior distributions is offered in
\parencite[Chap. 2.4]{Gelman2014_BayesianDataAnalysis}.

In real data analysis, a prior distribution that is not conjugate is often needed. In complex models that require
multiple prior distributions, represent some structure in data, or work with latent variables, conjugacy is often not
possible, and analytical solutions do not exist. To overcome this, simulation methods have been developed to sample
observations from a posterior distribution without the need to know the explicit analytical solutions. These methods do
not work with Bayes' theorem in @eq-bayes-eq but rather with its kernel

$$
P(\theta | x ) \propto P(\theta)P(x|\theta).
$$ {#eq-bayes-prop}

Because the predictive probability, $P(x)$, is constant and not dependent on the estimated parameters $\theta$, it is a
normalization constant that can be excluded. This version is represented in Equation @eq-bayes-prop, where $\propto$
means that the posterior distribution is proportional to the combination of prior distribution $P(\theta)$ and
likelihood of the data $P(x|\theta)$.

The choice of the prior distribution depends on the knowledge that is known before the analysis. If, for example, it is
known that a parameter is strictly positive, distributions such as the chi-squared, gamma, exponential, or
inverse-gamma can be used. Another approach might be to estimate some transformation of a parameter.
\parencite[Chap. 4.4]{McElreath2020_StatisticalRethinkingBayesian} chooses to model linear regression with $\ln
\beta_j$, which can have any sign. After estimation, the samples from logarithmic posterior distribution can be
exponentiated, which yields a positive effect of $x_j$ through $\exp \beta_j$. If the prior information is limited,
different approaches exist. One of them is the Jeffreys' invariance principle that states that *\enquote{any rule for
determining the prior density for $P(\theta)$ should yield an equivalent result if applied to a transformed parameter}*
\parencite[Chap. 2.8]{Gelman2014_BayesianDataAnalysis}.