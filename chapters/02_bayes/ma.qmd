The Metropolis Algorithm (MA) published in 1953 \parencite{MetropolisEtAl1953_EquationStateCalculations} has been very influential in the 
popularization of bayesian statistics because of it's simplicity and relative efficiency in bayesian models and is explored in
\parencite[Chap. 11.2]{Gelman2014_BayesianDataAnalysis} or \parencite[Chap. 7]{Kruschke2015_DoingBayesianData}. The algorithm first needs an
initial draw that will be used. Since the stationary distribution of Markov Chain does not depend on the starting value, it can be any
possible value whose posterior density is greater than 0. Then, a symmetric proposal distribution is required that generates new samples, conditional on 
the last observed sample, to create a Markov Chain. A popular choice can be normal distribution that is centered around the last sample with 
user defined variance, so

$$
\theta^* \sim N(\mu = \theta_{t-1}, \sigma^2).
$$

Variance is often denoted as a \enquote{step} that will be taken for the next sample. Large variance implies large jumps between individual samples,
while small variance can increase autocorrelation and the chain might not 
converge efficiently. The choice of variance hence depends on the specific model at hand. After a new sample is generated from the proposal 
distribution, it is compared to the last sample $\theta_{t-1}$ using a simple ratio of posterior densities

$$
r = \frac{
    P(\theta^*|y)
}{
    P(\theta^{t-1}|y)
},
$$ {#eq-ma}

where $P$ denotes the density of the posterior distribution for $\theta^*$, defined in Equation @eq-bayes-prop.
If the ratio is greater than 1, it means that the density of the proposed sample $\theta^*$ is larger than the density of $\theta_{t-1}$ and the sample is
accepted with probability of 1. If the ratio is smaller, the density of the proposed sample is smaller than the density of the last sample
and the probability of accepting the new sample is $r$. The decision whether the new sample is accepted or not can defined as

$$
\text{prob}(\theta_t = \theta^*) = \min(1, r).
$$

If the proposal is accepted, a new sample from the proposal distribution, condition on the new sample $\theta^*$ is generated. If not, a new sample 
$\theta_t$ is simply the previous sample $\theta_{t-1}$ and a new proposal conditioned on $\theta_{t-1}$ is generated.

While this procedure is capable of sampling from the posterior distribution, it might not always be the best choice. One of its main
problems is the efficiency in complex parameter spaces where the size of the step needs to be carefully tuned.
\parencite{MbalawataEtAl2013_AdaptiveMetropolisAlgorithm} notes that the simple MA can be greatly improved with a careful tuning of the
step size. If a Gaussian proposal distribution is used, \parencite{GelmanEtAl1996_EfficientMetropolisJumping} found that under specific settings,
the optimal covariance matrix $\Sigma$ that is defined by a researcher should be multiplied by a coefficient $\lambda = 2.38^2/d$, where $d$ is the dimension of the matrix.

To make the algorithm itself more efficient, special Adaptive Metropolis Algorithms have been developed that are able to either select
an optimal symmetrical proposal distribution or update the covariance matrix of such distribution. This approach allows the user to
more efficiently explore the parameter space of complex bayesian models. The final chains are more stable, suffer from smaller autocorrelation and
offer better ESS. The description of more adaptive algorithms, discussion of other algorithms and comparison can be found 
in \parencite{LiangEtAl2010_AdvancedMarkovChain}. 

An example of generated samples from the posterior distribution is in Figure @fig-sim-ma. These three charts show a simulation of the beta-binomial
model defined in Equation @eq-beta-binomial. Because in this model, the true posterior distribution is known, the accuracy of the simulation can be easily
compared. The line chart represents the true posterior distribution while the histogram the simulated data.

![Simulation of a beta-binomial model using the Metropolis algorithm.](./img/sim_ma.png){#fig-sim-ma}

The simulation generates 10 000 iterations with a burn-in period of 2 000 initial samples. The proposal distribution is symmetric normal 
distribution $N(\theta_{t-1}, \sigma = 1)$. For this simulation,
the generated proposals need to be in $\langle 0, 1 \rangle$, otherwise R throws an error because it is unable to calculate the density of
binomial distribution for values greater than 1 or smaller than 0. In smaller and higher true parameter values, the samples do not converge to the
true posterior distribution, yet. This may suggest that the variance is large, the symmetric proposal distribution does not fit the model well
or that another sampler is needed. The code producing Figure 2.1 can be found in Appendix \ref{code-ma}.
