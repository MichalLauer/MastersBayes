MCMC methods leverage a MH random walk through the parameter space to create samples that are arbitrary close to the posterior distribution. In
large, high dimensional models, this approach might struggle because the posterior parameter space. The structure might be so complex that a random
walk will take a very long time to explore all possible states, which is not efficient. Even with advanced techniques that implement adaptive
steps or change the proposal distribution, the convergence time is prohibitive \parencite[Chap. 12.4]{Gelman2014_BayesianDataAnalysis}.
This complexity might also prevent MCMC methods to adequately sample from regions with low density, and such, tails of the posterior
density might burdened with a relatively high MC error \parencite[Chap. 14.1]{Kruschke2015_DoingBayesianData}.

Hamiltonian Monte Carlo (HMC) methods do not rely on MH random walk, but with Hamiltonian mechanics. They offer a link between classical mechanics
and quantum mechanics. This domain is out of the scope of this thesis, but is explored in some physical textbooks, such as \parencite{Lowenstein2012_EssentialsHamiltonianDynamics}. Mathematical explanation of HMC is offered in \parencite[Chap. 12.4]{Gelman2014_BayesianDataAnalysis},
while an intuitive introduction to the topic is offered in \parencite[Chap. 14.1]{Kruschke2015_DoingBayesianData} or 
\parencite[Chap. 9.1]{McElreath2020_StatisticalRethinkingBayesian}.

HMC utilizes a set of momentum variables $\phi_i$ for every estimated parameter $\theta_i$, $i =
1, \dots, d$. Momentum variables represent a \enquote{jump} between every new sample and is completely independent of the estimated
parameter $\theta$. Then, the probability distribution of jumps $P(\phi)$, often called the jumping distribution, is combined with the
posterior distribution, and

$$
P(\phi, \theta | x) = P(\phi)P(\theta|x)
$$

is estimated. The jumping distribution is usually set to be a multivariate normal distribution $N_d(\mu, M)$, where $M$ denotes
a variance-covariance matrix, often called a mass matrix. If $M$ is a diagonal unit matrix, the distribution is a simple multivariate
standardized normal distribution. To make the algorithm more efficient, implementations with different mass matrix or with a different distribution exist. 

HMC also works with the gradient of the posterior log-density, which is often computed analytically using mathematical software. The gradient can
be defined as

$$
\frac{ \partial \log p(\theta | x) }{\partial \theta} =
\left( \frac{ \partial \log p(\theta | x) }{\partial \theta_1}, \dots, \frac{ \partial \log p(\theta | x) }{\partial \theta_d} \right ).
$$

First, the HMC algorithm draws initial values for $\phi$ from the jump distribution. Then, parameters $\theta$ and jumps $\phi$ are simultaneously
updated in multiple iterations using $L$ \enquote{leapfrog steps}, where each step is comprised of two parts. First, the jumps are updated with
the gradient as

$$
\phi_{t+1} = \phi_t + {1\over2} \epsilon \frac{ \partial \log p(\theta | x) }{\partial \theta},
$$

where $\epsilon$ denotes a hyperparameter that scales each step. Then, a new proposal is computed as

$$
\theta_{t+1} = \theta_t + \epsilon M^{-1} \phi.
$$

This process of leapfrog steps is repeated until the desired number of iterations. After $L$ iterations, new proposed
parameter $\theta^*$ and steps $\phi^*$ are obtained. New proposals are used to compute ratio

$$
r = \frac{
    P(\theta^*|x) P(\phi^*)
}{
    P(\theta_{t-1}|x) P(\phi_{t-1})
},
$$

where the proposals are accepted with probability $\min(1, r)$. After accepting or rejecting a new proposal $\theta^*$ and storing new sample
$\theta_t$, new jumps $\phi$ are generated from the jumping distribution and the process is repeated. As well as the MCMC algorithms, HMC
is guaranteed to converge to a specific stationary distribution that is the posterior distribution.

There are three ways how HMC can be made more efficient. Firstly, the jumping distribution can be fine-tuned for specific models which can increase
the efficiency of samples. Secondly, scales $\epsilon$ can be tuned to allow greater or smaller jumps and steps. Finally, the number of leap frogs $L$
can be optimized. As with MCMC, these parameters can be either set manually or adaptively. As \parencite[Chap. 12.4]{Gelman2014_BayesianDataAnalysis}
notes, changing the parameters mid-inference can lead to instability and the adaptive phase should be run only during the warmup phase.

Even though the method offers higher accuracy in small samples, it is computationally more costly, and MCMC methods could be better in relatively simple,
non-complex models. It is also limited to continuous parameters and imputation of discrete probability distributions is not possible[^hmc-disc] \parencite[Chap. 9.3]{McElreath2020_StatisticalRethinkingBayesian}. HMC also takes ideas from GS, specifically from the blocked approach to making samples.
HMC is able to sample highly correlated parameters jointly in blocks, which can make samples more efficient.

[^hmc-disc]: There are implementations that are able to do this. For example, a restructured model in Stan is able to identify such model. 
Examples are provided in \parencite[Chap 15., Chap. 16]{McElreath2020_StatisticalRethinkingBayesian}.